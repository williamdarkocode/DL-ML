{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit ('ai': conda)"
  },
  "interpreter": {
   "hash": "ebce0c7dac342d90580125ce713960370f01d7bc21030aebdc37b54c0352bd57"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "author: William Darko (repurposed from original author Francois Chollet)\n",
    "date: July, 2021\n",
    "description: Regression problem using keras provided Boston housing dataset; example taken, an repurposed from 'Deep Learning with Python' (Manning publication, 1st ed) book by Francois Chollet\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras.datasets import boston_housing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading, and inspecting the data\n",
    "\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
    "\n",
    "print(\"Training data shape: \", train_data.shape)\n",
    "print(\"Training targets shape: \", train_targets.shape)\n",
    "print(train_data[:10])\n",
    "\n",
    "# train_data.shape[0] corresponds to number of samples, in this case 404\n",
    "# train_data.shape[1] corresponds to number of features per sample, in this case 13\n",
    "# each sample represents a particular suburb in the Boston area, with 13 features such as crime rate, etc.\n",
    "# targets represent median values of homes in thousands of dollars per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalising the data (feature-wise normalisaiton of data)\n",
    "# because features values take wildy different ranges, this would make learning more difficult, thus feature normalisation is required to\n",
    "# centre values around 0, and have unit standard deviations. This requires taking the mean of each feature column and for each feature subtracting the mean, and dividing by the standard deviation. This method is also recognised as Standardisation, or Z-score\n",
    "\n",
    "feature_mean = train_data.mean(axis=0)\n",
    "# pass axis=0 for the mean to be computed along the columns of the matrix (per feature column)\n",
    "\n",
    "train_data-=feature_mean\n",
    "stdev = train_data.std(axis=0)\n",
    "train_data/=stdev\n",
    "# train data is now a matrix of z-scores\n",
    "\n",
    "test_data-=feature_mean\n",
    "test_data/=stdev\n",
    "# notice we use same mean, and standard deviation computed on training data; never use quantities computed on testing data. not even for normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition, and compilation\n",
    "# because we'll be using k-fold validation, since the training data isn't enough to just split into a partial training set, and a validation set, we'll be partitionaling data according to k-folds cross validation, thus the model would have to be instantiated for each training partition.\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))\n",
    "    # shape[1] corresponding to the number of features\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    # no activation used in the final layer to not restrict the range of outputs. A linear layer\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    # mse (mean squared error) for loss function, and mae (mean absolute error) for metrics\n",
    "    # mse is the square of the difference between predictions and targets\n",
    "    # mae is absolute value of difference between predictions and targets\n",
    "    return model\n",
    "# typical scalar regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# k-folds partitioning and cross validation\n",
    "\n",
    "k = 4 # 4-fold validation\n",
    "num_validation_samples = len(train_data) // k\n",
    "# print(\"Length of train_data: {}, num validation samples {}\".format(len(train_data), num_validation_samples))\n",
    "\n",
    "num_epochs = 100\n",
    "# all_scores = []\n",
    "all_mae_histories = []\n",
    "\n",
    "for i in range(k):\n",
    "    print('processing fold #: ', i)\n",
    "    validation_data = train_data[i * num_validation_samples: (i+1)*num_validation_samples]\n",
    "    validation_targets = train_targets[i * num_validation_samples: (i+1)*num_validation_samples]\n",
    "    print(\"i: {} , val data & targets: [{}:{}]\".format(i, i*num_validation_samples, (i+1)*num_validation_samples))\n",
    "\n",
    "    partial_train_data = np.concatenate( [train_data[:i*num_validation_samples], train_data[(i+1)*num_validation_samples:]], axis=0 )\n",
    "\n",
    "    partial_train_targets = np.concatenate( [train_targets[:i*num_validation_samples], train_targets[(i+1)*num_validation_samples:]], axis=0 )\n",
    "\n",
    "    model = build_model()\n",
    "    # model.fit(partial_train_data, partial_train_targets, epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    # validation_mean_squared_error, validation_mean_absolute_error = model.evaluate(validation_data, validation_targets, verbose=0)\n",
    "\n",
    "    # all_scores.append(validation_mean_absolute_error)\n",
    "    num_epochs = 500\n",
    "    history = model.fit(partial_train_data, partial_train_targets, epochs=num_epochs, batch_size=1, verbose=0, validation_data=(validation_data, validation_targets))\n",
    "    # print(history.history.keys())\n",
    "    mean_abs_error_history = history.history['mae']\n",
    "    all_mae_histories.append(mean_abs_error_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building history of successive mean k-fold validation scores & plotting them\n",
    "avg_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
    "\n",
    "plt.plot(range(1, len(avg_mae_history) + 1), avg_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothing curve; omitting first 10 data points which are on a different scale than rest of graph"
   ]
  }
 ]
}